{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instalação das bibliotecas:\n"
      ],
      "metadata": {
        "id": "XCOzXoiSPfOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY98rV5Cq1fL"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-llms-huggingface\n",
        "%pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index ipywidgets\n",
        "!pip install pypdf transformers einops accelerate langchain bitsandbytes sentence-transformers\n",
        "!pip install gradio\n",
        "!pip install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "gWL51odzcrIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login na Hugging Face:\n",
        "\n",
        "*Necessário para acessar os modelos de linguagem hospedados no Hugging Face.*"
      ],
      "metadata": {
        "id": "rrLeFlySP55F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jwqAuAfrBHB"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação de bibliotecas:"
      ],
      "metadata": {
        "id": "Mu6wdgSaQHGU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQCSO4A5qd-6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLAMA2_13B_CHAT é o nome do modelo que será utilizado. O system_prompt define as instruções que guiarão o comportamento do assistente virtual, especificando que ele deve responder em português e fornecer explicações detalhadas."
      ],
      "metadata": {
        "id": "fmzeLcKsQOtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh3ykWrxqZvI"
      },
      "outputs": [],
      "source": [
        "# Configurar o modelo e tokenizer\n",
        "LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "system_prompt = \"\"\"\n",
        "Você é um assistente virtual especializado em ajudar estudantes com perguntas sobre diversos tópicos acadêmicos, incluindo negócios, tecnologia, biologia, ciências, e matemática. Responda SEMPRE em português, independentemente da pergunta ou do contexto, de forma clara e concisa. Forneça explicações detalhadas e exemplos práticos, se necessário.\n",
        "\n",
        "Se uma pergunta não fizer sentido ou não for factualmente coerente, explique o porquê de forma educada, ao invés de responder algo incorreto. Se você não souber a resposta para uma pergunta, não compartilhe informações falsas.\n",
        "\n",
        "Para perguntas acadêmicas, forneça explicações completas. Não forneça recomendações de coisas que não existem.\n",
        "\n",
        "Seja sempre útil, respeitoso, e educado em suas respostas. Caso um aluno seja desrespeitoso, responda de forma educada.\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = PromptTemplate(\n",
        "    \"[INST]<<SYS>>\\n\" + system_prompt + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
        ")\n",
        "\n",
        "# Carregar o modelo e tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA2_13B_CHAT)\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLAMA2_13B_CHAT,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Configurar LLM\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=500,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=LLAMA2_13B_CHAT,\n",
        "    model_name=LLAMA2_13B_CHAT,\n",
        "    model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tPfABOTqjuR"
      },
      "outputs": [],
      "source": [
        "Settings.llm = llm  # Configurar o LLM\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")  # Configurar o modelo de embeddings\n",
        "\n",
        "# Carregar documentos\n",
        "documents = SimpleDirectoryReader(\"materiais\").load_data()\n",
        "\n",
        "# Criar o índice\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Criar mecanismo de consulta\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o092AEQjJLvU"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Instanciar o tradutor\n",
        "translator = GoogleTranslator(source='auto', target='pt')\n",
        "\n",
        "# Função para verificar e traduzir para português\n",
        "def translate_to_portuguese(text):\n",
        "    detected_lang = detect(text)\n",
        "    if detected_lang != 'pt':\n",
        "        # Traduzir para o português\n",
        "        translated = translator.translate(text)\n",
        "        return translated\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interface Gradio:"
      ],
      "metadata": {
        "id": "UVnJAyOwSlIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46zDL3ffqoWZ"
      },
      "outputs": [],
      "source": [
        "# Função para consultar o `query_engine` e gerar uma resposta\n",
        "def chatbot_response(message, history):\n",
        "    response = query_engine.query(message)\n",
        "\n",
        "    # Garantir que a resposta esteja em português\n",
        "    response_in_portuguese = translate_to_portuguese(str(response))\n",
        "    return response_in_portuguese\n",
        "\n",
        "# Criar a interface Gradio no estilo de um chatbot\n",
        "chatbot = gr.ChatInterface(fn=chatbot_response,\n",
        "                           title=\"Assistente Virtual\",\n",
        "                           description=\"Converse com o chatbot e receba respostas para suas perguntas!\",\n",
        "                           theme=\"compact\")\n",
        "\n",
        "# Iniciar a interface Gradio com compartilhamento\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}